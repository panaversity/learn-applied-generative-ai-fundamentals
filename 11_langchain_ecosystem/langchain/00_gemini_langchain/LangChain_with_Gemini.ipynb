{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Google Gemini API with LangChain Project**"
      ],
      "metadata": {
        "id": "Sl9hmymDl0kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Environment Setup and Utility Functions**"
      ],
      "metadata": {
        "id": "kU16yqzE84QT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Install Required Package**\n",
        "\n",
        "**LangChain:** A framework for building applications using large language models, facilitating the creation of language model pipelines.\n",
        "\n",
        "**LangChain Google GenAI Integration:** An integration with Google's Generative AI tools, allowing for the use of advanced language models within the LangChain framework."
      ],
      "metadata": {
        "id": "IOBAlT2al7N9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts9Ro5dgQevN",
        "outputId": "cb9061d9-52d3-478e-c863-5bcbc4241fc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m515.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.2/290.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install the LangChain and LangChain's Google GenAI integration\n",
        "# `-q` keeps the output minimal.\n",
        "# `-U` ensures you are using the latest versions of the packages\n",
        "%pip install -q -U langchain\n",
        "%pip install -q -U langchain-google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Resetting the Jupyter Notebook Kernel**"
      ],
      "metadata": {
        "id": "cFFl37bP6PDq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OaFyyFGonc3",
        "outputId": "662efd2a-d500-456a-a62c-d096298be300"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Import the IPython library to access its application instance\n",
        "import IPython\n",
        "\n",
        "# Retrieve the current IPython application instance\n",
        "app = IPython.Application.instance()\n",
        "\n",
        "# Perform a complete shutdown of the current IPython kernel including restarting the kernel\n",
        "# it will help the environment to access the new packages\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Defining Helper Functions**\n",
        "\n",
        "When a model returns simple text, it often lacks the formatting needed to make the content easily digestible. By using markdown formatting, we can enhance the presentation of this plain text, transforming it into a structured and visually appealing format. For instance, converting bullet points into asterisks creates clear lists, while indenting text as blockquotes emphasizes important information. This improved formatting not only enhances readability but also helps users engage more effectively with the content, allowing them to grasp key ideas quickly and easily. Overall, proper formatting is essential for conveying information clearly and making the user experience more enjoyable."
      ],
      "metadata": {
        "id": "yPgxlQ3Z8GGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the textwrap module for text formatting and indentation\n",
        "import textwrap\n",
        "\n",
        "# Import the Markdown display function from IPython to render text as Markdown in Jupyter Notebooks\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Define a function 'to_markdown' that converts a given text into Markdown format\n",
        "def to_markdown(text) -> Markdown:\n",
        "    # Replace bullet points (•) with Markdown-compatible bullet points (*)\n",
        "    text: str = text.replace(\"•\", \"  *\")\n",
        "\n",
        "    # Indent the entire text block with the Markdown blockquote symbol ('> ')\n",
        "    # The lambda function ensures every line is indented\n",
        "    return Markdown(textwrap.indent(text, \"> \", predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "S-OFWytF8Lk_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Method 01: Using API key for auth**"
      ],
      "metadata": {
        "id": "g6AMbuuLmiiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Get your API key**\n",
        "\n",
        "Before you can use the Gemini API, you must first obtain an API key. If you don't already have one, create a key with one click in Google AI Studio.\n",
        "\n",
        "<a class=\"button button-primary\" href=\"https://makersuite.google.com/app/apikey\" target=\"_blank\" rel=\"noopener noreferrer\">Get an API key</a>"
      ],
      "metadata": {
        "id": "dr_83mNl9rd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Access your API key in colab**"
      ],
      "metadata": {
        "id": "3MUc_0OP-AT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing userdata from Google Colab to securely store and access API keys\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "YBmxOFrh-MpN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After obtaining the API key, you can access it in Colab\n",
        "\n",
        "* Set the key in the GEMINI_API_KEY environment variable.\n",
        "* You can save this API key under any variable name you prefer.\n",
        "* Remember to enable access for the saved API key in Colab using the toggle button."
      ],
      "metadata": {
        "id": "rotsgU8f-oYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# after saving api key in env variables\n",
        "# get api key from env\n",
        "google_api_key = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "0NL4LdG4mrj0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Initializing LangChain with GEMINI for AI Chat Responses**"
      ],
      "metadata": {
        "id": "omdHGG_n_Mrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the ChatGoogleGenerativeAI class from the langchain_google_genai module\n",
        "# this will be used for using langchain with gemni\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Import the AIMessage class currently will be used for typing\n",
        "from langchain_core.messages.ai import AIMessage\n",
        "\n",
        "# Initialize an instance of the ChatGoogleGenerativeAI with specific parameters\n",
        "llm: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",  # Specify the model to use\n",
        "    api_key=google_api_key,     # Provide the Google API key for authentication\n",
        "    temperature=0.2,            # Set the randomness of the model's responses (0 = deterministic, 1 = very random)\n",
        ")"
      ],
      "metadata": {
        "id": "3-PWGEjH_8Q9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Invoking LangChain Model with Prompt to Get Response**"
      ],
      "metadata": {
        "id": "6nlhwHPDAhF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the LangChain model with a prompt to generate a response\n",
        "ai_msg: AIMessage = llm.invoke(\"What is the capital of France?\")"
      ],
      "metadata": {
        "id": "JCLYKhCynBtr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display complete response\n",
        "ai_msg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUAhJY2lpn-7",
        "outputId": "2c7c0619-f06d-4858-e8a2-f44d820ed2a2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The capital of France is **Paris**. \\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-b4e50199-9716-4287-b0d0-e3ee715990b5-0', usage_metadata={'input_tokens': 8, 'output_tokens': 8, 'total_tokens': 16})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get actual response\n",
        "ai_msg.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "idD2RwpqpqoU",
        "outputId": "8876fe65-e6c1-4b36-8ace-e51c83f7ef58"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The capital of France is **Paris**. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# format the response with markdown\n",
        "to_markdown(ai_msg.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "id": "w4VJvxUZA7Ce",
        "outputId": "ddbecc87-f65b-49fb-b9f3-80b74e680605"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> The capital of France is **Paris**. \n"
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Method 02: Using JSON file for Auth**"
      ],
      "metadata": {
        "id": "KIB0oYCvpy0l"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o97VoIzdXGy"
      },
      "source": [
        "### **Creating a Json file for Auth or allowing Gemini api in Langchain**\n",
        "Adding Api key to environment variables, and getting it's value sometime cause auth issues, In case of facing Auth Issues, Follow the following steps if you are getting Auth related errors in getting response from Gemini, and being asked for auth.\n",
        "  - Open [Google Cloud Console](https://console.cloud.google.com/), at the top left corner, click on **Select a Project**, a new Window screen will pop up, Select an existing one or create a new Project. (Free Version).\n",
        "  - After Selecting or Creating a project, below **WELCOME** screen, Select **APIs and Services** from **Quick Access**.\n",
        "  - On *APIs and Services* windows, look for Library and left Sidebar. Select **Library**.\n",
        "  - In search box, type **Gemini Api**, two results will be shown,\n",
        "      1. *Gemini Api*\n",
        "      2. *Gemini for google cloud*\n",
        "\n",
        "  Select first one. Click on **Enable**. Gemini Api will be enabled.\n",
        "\n",
        "  - Now get back to **APIs and and Services** Window.\n",
        "  - Select **Credentials**, at Top level, after Credentials, Select **Create Credentail**, and click on API Key from the new dropdown. It will generate an api key.\n",
        "  - Click on **Google Cloud**, at top left corner. It will take you to Welcome page.\n",
        "  - From **Quick Access**, Select **IAM and Admin**.\n",
        "  - At left sidebar select **Service accounts** and click on **Create Service Account**\n",
        "      - Fill in the required Details, click on **Create and Continue**.\n",
        "      - Click on **Select Role**, search of *OWNER*, and select the one with full previlages.\n",
        "      - Click on **Continue**.\n",
        "      - Skip the fields, click on **Done**.\n",
        "      - At *Service Accounts* windows, click on the Email, we just created.\n",
        "      - Click the Newly generated Email.\n",
        "      - At the top, below the Key name where it's mentioned, Click on **KEYS**, from the taskbar above,\n",
        "      - At **Keys** window, select **Add Key**.\n",
        "          - Click on **Create new Key**.\n",
        "          - Select **Add Key**.\n",
        "          - Make sure **JSON** is selected.\n",
        "          - Click on **Create Key**. A New dialogue window will pop up telling you that **Private key saved to your computer**\n",
        "          - The .json file is downloaded to your default download folder.\n",
        "Upload it to the root directory of Colab.\n",
        "          - Right click on the newly add file, select **Copy Path**. Replace it with __your path__ in below cell."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Setting Up Google Cloud Application Credentials**\n",
        "\n",
        "Configures the environment by setting the GOOGLE_APPLICATION_CREDENTIALS variable. This variable points to the JSON file containing the necessary credentials for authenticating with Google Cloud services. By doing this, the application can securely access Google Cloud resources using the specified credentials."
      ],
      "metadata": {
        "id": "ExG5DkcCCN1j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gZnwV44lr7MS"
      },
      "outputs": [],
      "source": [
        "# Import the os module to interact with the operating system\n",
        "import os\n",
        "\n",
        "# Set the environment variable for Google Cloud application credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"Your path goes here\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Initializing LangChain with GEMINI for AI Chat Responses**"
      ],
      "metadata": {
        "id": "k0Bd540kCtmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the ChatGoogleGenerativeAI class from the langchain_google_genai module\n",
        "# this will be used for using langchain with gemni\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Import the AIMessage class currently will be used for typing\n",
        "from langchain_core.messages.ai import AIMessage\n",
        "\n",
        "# Initialize an instance of the ChatGoogleGenerativeAI with specific parameters\n",
        "# as we are using .json file auth in this case we don't need to specify api key here\n",
        "llm: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",  # Specify the model to use\n",
        "    temperature=0.2,            # Set the randomness of the model's responses (0 = deterministic, 1 = very random)\n",
        ")"
      ],
      "metadata": {
        "id": "M0Jr4u-pCw-O"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Invoking LangChain Model with Prompt to Get Response**"
      ],
      "metadata": {
        "id": "v-NA-QtMDAwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the LangChain model with a prompt to generate a response\n",
        "ai_msg : AIMessage = llm.invoke(\"What is the capital of France?\")"
      ],
      "metadata": {
        "id": "zbr5PHvmC_iq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display complete response\n",
        "ai_msg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TceZg7A6DU7D",
        "outputId": "541eb029-35e2-4f53-c798-dd8e995df0c9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The capital of France is **Paris**. \\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-6b7f17f7-5e3d-457e-ac5a-cf30ce8c95fa-0', usage_metadata={'input_tokens': 8, 'output_tokens': 8, 'total_tokens': 16})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get actual response\n",
        "ai_msg.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KQIvv5RkDVVD",
        "outputId": "32aa27ed-17ab-4d5a-93b0-85b862f6a380"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The capital of France is **Paris**. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# format the response with markdown\n",
        "to_markdown(ai_msg.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "id": "_aROZmJfDZri",
        "outputId": "13a53c90-ccb8-4656-e362-3d67a807a227"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> The capital of France is **Paris**. \n"
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Invoking LangChain Model with Structured Messages for AI Responses**"
      ],
      "metadata": {
        "id": "R7ihQpV-DnTv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JA4RDvoNkmlk"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "message : list[Dict[str:str]] = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Which open source AI Model is best so far\"},\n",
        "]\n",
        "\n",
        "ai_msg = llm.invoke(message)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ai_msg.content)"
      ],
      "metadata": {
        "id": "Cio28NOyt0KA",
        "outputId": "9a6d4dc4-4869-4648-bf64-dd328a7356fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It's impossible to definitively say which open-source AI model is \"best\" because the best model depends on your specific needs and use case. \n",
            "\n",
            "Here's a breakdown of some popular open-source models and their strengths:\n",
            "\n",
            "**For Text Generation:**\n",
            "\n",
            "* **GPT-Neo:** A powerful language model from EleutherAI, known for its impressive text generation capabilities. It comes in various sizes, offering a trade-off between performance and computational resources.\n",
            "* **BLOOM:** A large language model developed by BigScience, trained on a massive dataset of text and code. It excels in multilingual tasks and code generation.\n",
            "* **OPT:** A large language model from Meta AI, designed to be more efficient and accessible than other models. It's a good choice for research and experimentation.\n",
            "* **StableLM:** A family of language models from Stability AI, known for their ability to generate creative and engaging text.\n",
            "\n",
            "**For Image Generation:**\n",
            "\n",
            "* **Stable Diffusion:** A powerful text-to-image generation model, capable of creating stunning and realistic images from text prompts.\n",
            "* **DALL-E 2:** While not strictly open-source, DALL-E 2's API allows for integration with external applications. It's known for its high-quality image generation and creative capabilities.\n",
            "* **Imagen:** Another powerful text-to-image generation model from Google AI, known for its ability to generate photorealistic images.\n",
            "\n",
            "**For Multimodal Tasks:**\n",
            "\n",
            "* **BLIP:** A model that combines image and text understanding, allowing it to perform tasks like image captioning and visual question answering.\n",
            "* **Flamingo:** A model that can understand and reason about images and text, making it suitable for tasks like image retrieval and visual dialogue.\n",
            "\n",
            "**Factors to Consider When Choosing a Model:**\n",
            "\n",
            "* **Task:** What specific task do you want to perform?\n",
            "* **Data:** What kind of data will you be using?\n",
            "* **Resources:** How much computational power and memory do you have available?\n",
            "* **Performance:** What level of accuracy and quality do you need?\n",
            "* **Ease of Use:** How easy is the model to set up and use?\n",
            "\n",
            "**Where to Find Open-Source Models:**\n",
            "\n",
            "* **Hugging Face:** A popular platform for sharing and using open-source AI models.\n",
            "* **GitHub:** A code repository where you can find many open-source AI projects.\n",
            "* **Papers with Code:** A website that lists and links to open-source AI models and their corresponding research papers.\n",
            "\n",
            "**Recommendation:**\n",
            "\n",
            "Instead of focusing on finding the \"best\" model, it's more helpful to explore different models and choose the one that best suits your specific needs and use case. Experiment with different models and see which one performs best for your task.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(ai_msg.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "d3nc1f0mE943",
        "outputId": "4771c327-bf6f-430c-92b2-676c056902f4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> It's impossible to definitively say which open-source AI model is \"best\" because the best model depends on your specific needs and use case. \n> \n> Here's a breakdown of some popular open-source models and their strengths:\n> \n> **For Text Generation:**\n> \n> * **GPT-Neo:** A powerful language model from EleutherAI, known for its impressive text generation capabilities. It comes in various sizes, offering a trade-off between performance and computational resources.\n> * **BLOOM:** A large language model developed by BigScience, trained on a massive dataset of text and code. It excels in multilingual tasks and code generation.\n> * **OPT:** A large language model from Meta AI, designed to be more efficient and accessible than other models. It's a good choice for research and experimentation.\n> * **StableLM:** A family of language models from Stability AI, known for their ability to generate creative and engaging text.\n> \n> **For Image Generation:**\n> \n> * **Stable Diffusion:** A powerful text-to-image generation model, capable of creating stunning and realistic images from text prompts.\n> * **DALL-E 2:** While not strictly open-source, DALL-E 2's API allows for integration with external applications. It's known for its high-quality image generation and creative capabilities.\n> * **Imagen:** Another powerful text-to-image generation model from Google AI, known for its ability to generate photorealistic images.\n> \n> **For Multimodal Tasks:**\n> \n> * **BLIP:** A model that combines image and text understanding, allowing it to perform tasks like image captioning and visual question answering.\n> * **Flamingo:** A model that can understand and reason about images and text, making it suitable for tasks like image retrieval and visual dialogue.\n> \n> **Factors to Consider When Choosing a Model:**\n> \n> * **Task:** What specific task do you want to perform?\n> * **Data:** What kind of data will you be using?\n> * **Resources:** How much computational power and memory do you have available?\n> * **Performance:** What level of accuracy and quality do you need?\n> * **Ease of Use:** How easy is the model to set up and use?\n> \n> **Where to Find Open-Source Models:**\n> \n> * **Hugging Face:** A popular platform for sharing and using open-source AI models.\n> * **GitHub:** A code repository where you can find many open-source AI projects.\n> * **Papers with Code:** A website that lists and links to open-source AI models and their corresponding research papers.\n> \n> **Recommendation:**\n> \n> Instead of focusing on finding the \"best\" model, it's more helpful to explore different models and choose the one that best suits your specific needs and use case. Experiment with different models and see which one performs best for your task.\n"
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "asRE7om_FbVf"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}