{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# First, install necessary packages for LangChain and Google Generative AI\n",
        "!pip install langchain_core\n",
        "!pip install langchain_google_genai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2GYbrBvnvEQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve your Google API key from a secure location (Colab secret storage)\n",
        "# Replace 'YOUR_SECRET_NAME' with the name of the environment variable storing your API key\n",
        "google_api_key: str = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "wFSgVJ4dvNxB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Create an instance of the Google Generative AI model (LLM)\n",
        "# - 'api_key' is required to authenticate with Google's API\n",
        "# - 'model' specifies the version of the AI model to use (gemini-1.5-flash in this case)\n",
        "# - 'temperature' controls the randomness of the responses (lower values = more focused responses)\n",
        "llm: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n",
        "    api_key=google_api_key,\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.2,\n",
        ")"
      ],
      "metadata": {
        "id": "KLWNhgjw1zrk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model by sending a simple \"hi\" message and print the AI's response\n",
        "# The 'invoke' method is used to query the model and get a response\n",
        "response: str = llm.invoke(\"hi\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkORDhrU__e3",
        "outputId": "00a39bc2-778f-445a-f1e2-7bf41c3d4f2e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Hi there! How can I help you today? \\n' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-89af641a-5e89-42e3-b0a8-648d0ee614d1-0' usage_metadata={'input_tokens': 2, 'output_tokens': 11, 'total_tokens': 13}\n"
          ]
        }
      ]
    }
  ]
}